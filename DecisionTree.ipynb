{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#for chisquare\n",
    "from scipy.stats import chi2\n",
    "\n",
    "#we are reading the csv\n",
    "totaldataset = pandas.read_csv(\"training.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#coverting the DataFrame to a Numpy array\n",
    "dataset_numpy = totaldataset.to_numpy()\n",
    "#deleting the first column \n",
    "matrix = np.delete(dataset_numpy, 0, 1)\n",
    "#splitting the dna into 60 and appending labels to that list forming a list of 61 elements\n",
    "reshape_numpy = []\n",
    "for row in matrix:\n",
    "    dna_seq = row[0]\n",
    "    dna_list = list(dna_seq)\n",
    "    dna_list.append(row[1])\n",
    "    reshape_numpy.append(dna_list)\n",
    "\n",
    "np_dataset = np.array(reshape_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(column): #function to find impurity via gini_index at parent\n",
    "    # we find all the unique lables and their counts\n",
    "    labels, num_labels = np.unique(column,return_counts = True)\n",
    "    total_gini = 0\n",
    "    gini_label = 0\n",
    "    #for each label we find the gini, sum them up and do 1 - the sum\n",
    "    for i in range(len(num_labels)):\n",
    "        percentage_of_label = num_labels[i] / len(column)\n",
    "        gini_label += percentage_of_label**2\n",
    "    total_gini = 1 - gini_label\n",
    "    return total_gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misclassification_error(column): #function to find impurity via misclassification_error at parent\n",
    "    # we find all the unique lables and their counts\n",
    "    labels, num_labels = np.unique(column,return_counts = True)\n",
    "    total_misclass = 0\n",
    "    percentage_of_label=[]\n",
    "    #for each label we find misclassication_error and do 1 - the maximum \n",
    "    for i in range(len(num_labels)):\n",
    "        ind_percentage_of_label = num_labels[i] / len(column)\n",
    "        percentage_of_label.append(ind_percentage_of_label)\n",
    "    misclass_label = np.max(percentage_of_label)\n",
    "    total_misclass = 1 - misclass_label\n",
    "    return total_misclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(column): #function to find impurity via entropy at parent\n",
    "    # we find all the unique lables and their counts \n",
    "    labels, num_labels = np.unique(column,return_counts = True)\n",
    "    total_entropy = 0\n",
    "    #for each label we find entropy and sum them up\n",
    "    for i in range(len(num_labels)):\n",
    "        percentage_of_label = num_labels[i] / len(column)\n",
    "        entropy_label = (-1) * percentage_of_label * np.log2(percentage_of_label)\n",
    "        total_entropy += entropy_label\n",
    "    return total_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InformationGain(current_dataset, column_name, class_label, method=\"gini\"): #function to find information gain via diff methods.\n",
    "    \n",
    "    #We created a nested dictionary that holds the values and counts of the char_val i.e.., 'A','G'...and the char_class_label i.e.., 'N', 'IE', 'EI'\n",
    "    \n",
    "    hash_map = {}\n",
    "    for index, char_val in np.ndenumerate(current_dataset[:, column_name]):\n",
    "        value_index = index[0]\n",
    "        char_class_label = current_dataset[:, class_label][value_index]\n",
    "        if char_val not in hash_map:\n",
    "            hash_map[char_val] = {}\n",
    "            if char_class_label not in hash_map[char_val]:\n",
    "                hash_map[char_val][char_class_label] = 1\n",
    "            else:\n",
    "                hash_map[char_val][char_class_label] += 1\n",
    "        else:\n",
    "            if char_class_label not in hash_map[char_val]:\n",
    "                hash_map[char_val][char_class_label] = 1\n",
    "            else:\n",
    "                hash_map[char_val][char_class_label] += 1\n",
    "    \n",
    "    #Based on the method called one of entropy, gini, or misclass will be executed.\n",
    "    if method == \"gini\":\n",
    "        total_gini = gini_index(current_dataset[:, class_label])\n",
    "        sum_gini =0\n",
    "        # for each char in the map, we find the gini and sum it all up\n",
    "        for char in hash_map.keys():\n",
    "            labels_counts = hash_map[char].values() #hash map which stores counts of chars \n",
    "            char_gini = 0\n",
    "            gini_char_label = 0\n",
    "            for char_label_count in labels_counts:\n",
    "                percentage_char_label = char_label_count / sum(labels_counts) #percentage of a label in a char(eg: %N)\n",
    "                gini_char_label+= percentage_char_label**2 \n",
    "            char_gini = 1- gini_char_label\n",
    "            sum_gini += sum(labels_counts)/ len(current_dataset[:, column_name]) * char_gini\n",
    "        Information_Gain = total_gini - sum_gini\n",
    "        return Information_Gain\n",
    "    \n",
    "    elif method == \"entropy\":\n",
    "        total_entropy = entropy(current_dataset[:, class_label])\n",
    "        sum_entropy = 0\n",
    "        # for each char in the map, we find the entropy and sum it all up\n",
    "        for char in hash_map.keys():\n",
    "            labels_counts = hash_map[char].values() #hash map which stores counts of chars\n",
    "            char_entropy = 0\n",
    "            for char_label_count in labels_counts:\n",
    "                percentage_char_label = char_label_count / sum(labels_counts) #percentage of a label in a char(eg: %N)\n",
    "                entropy_char_label = (-1) * percentage_char_label * np.log2(percentage_char_label)\n",
    "                char_entropy += entropy_char_label\n",
    "            sum_entropy += sum(labels_counts)/ len(current_dataset[:, column_name]) * char_entropy\n",
    "        Information_Gain = total_entropy - sum_entropy\n",
    "        return Information_Gain\n",
    "    \n",
    "    else:\n",
    "        if method ==\"misclass\":\n",
    "            total_misclass = misclassification_error(current_dataset[:, class_label])\n",
    "            sum_misclass =0\n",
    "            # for each char in the map, we find the misclass and sum it all up\n",
    "            for char in hash_map.keys():\n",
    "                labels_counts = hash_map[char].values()#hash map which stores counts of chars\n",
    "                char_misclass = 0\n",
    "                percentage_char_label=[]\n",
    "                for char_label_count in labels_counts:\n",
    "                    ind_percentage_char_label = char_label_count / sum(labels_counts) #percentage of a label in a char(eg: %N)\n",
    "                    percentage_char_label.append(ind_percentage_char_label)\n",
    "                misclass_char_label = np.max(percentage_char_label)\n",
    "                char_misclass =1-misclass_char_label\n",
    "                sum_misclass += sum(labels_counts)/ len(current_dataset[:, column_name]) * char_misclass\n",
    "            Information_Gain = total_misclass - sum_misclass\n",
    "            return Information_Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChiSquare(current_dataset, column_name, class_label=60): #find chiSquare for a column\n",
    "    result_chi = 0\n",
    "    unique_chars = np.unique(current_dataset[:, column_name]) #find unique chars ('A','G'..) \n",
    "    for char in unique_chars: #take a certain char and loop through\n",
    "        branch_dataset = current_dataset[current_dataset[:, column_name] == char] \n",
    "        #collect all the counts of unique labels of that char\n",
    "        branch_class_names, branch_class_counts = np.unique(branch_dataset[:, class_label], return_counts=True)\n",
    "        for cls in branch_class_names:\n",
    "            cls_index = list(branch_class_names).index(cls)\n",
    "            cls_count = branch_class_counts[cls_index]\n",
    "            real_n_count = cls_count #get the real count of labels that belong to char\n",
    "            left_expected = len(branch_dataset) # total branch length \n",
    "            #get the counts of the parent classes for expected count calculation\n",
    "            parent_cls_names, parent_cls_count = np.unique(current_dataset[:, class_label], return_counts=True)\n",
    "            parent_n_index = list(parent_cls_names).index(cls)\n",
    "            right_expected = parent_cls_count[parent_n_index] / len(current_dataset)\n",
    "            expected = left_expected * right_expected\n",
    "            chi = (real_n_count - expected) ** 2 / expected\n",
    "            result_chi += chi\n",
    "    \n",
    "    freedom = (len(unique_chars) - 1 ) * (len(np.unique(current_dataset[:, class_label])) -1 ) #degrees of freedom \n",
    "    critical_value = chi2.ppf(0.99, freedom) #calc crit value based on freedom and confidence\n",
    "    if result_chi > critical_value:\n",
    "        keep_building = True\n",
    "    else:\n",
    "        keep_building = False\n",
    "    return keep_building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID3Algorithm(current_dataset, completedataset, characteristics, labels_column=60, upper_node_label = \"N\"): #we are considering each column positon as a characteristics\n",
    "    #for the current data if there is only one label in the 60th column, then return that\n",
    "    if len(np.unique(current_dataset[:, labels_column])) <= 1:\n",
    "        return np.unique(current_dataset[:, labels_column])[0]\n",
    "    #if the len of the data is zero then return label which has the highest count in the complete data\n",
    "    elif len(current_dataset)==0:\n",
    "        return np.unique(completedataset[:, labels_column])[\n",
    "            np.argmax(np.unique(completedataset[:, \n",
    "                                             labels_column],return_counts=True)[1])]\n",
    "    #if there are no more features, then we return the label of the parent node\n",
    "    elif len(characteristics) ==0:\n",
    "        return upper_node_label\n",
    "    else:\n",
    "        upper_node_label = np.unique(current_dataset[:, labels_column])[\n",
    "            np.argmax(np.unique(current_dataset[:, labels_column],return_counts=True)[1])]\n",
    "        #ig_set has the list of all IG of all characteristics\n",
    "        ig_set = [InformationGain(current_dataset,characteristic,labels_column,\"misclass\") for characteristic in characteristics]\n",
    "        #top_characteristic_index has the index of the highest value of ig_set\n",
    "        top_characteristic_index = np.argmax(ig_set)\n",
    "        #we find the top_characteristic from the list using index\n",
    "        top_characteristic = characteristics[top_characteristic_index]\n",
    "        #we add that characteristic to tree\n",
    "        tree = {top_characteristic:{}}\n",
    "        #A list characteristics puts in the characteristics as long as it's not equal to the best characteristic\n",
    "        characteristics = [i for i in characteristics if i != top_characteristic]\n",
    "        #Check is it is noteworthy to keep building the tree by running the chisquare test\n",
    "        if (ChiSquare(current_dataset, top_characteristic)):\n",
    "        #if we need to continue building then we make a subtree which has sub dataset and continue it to iterate till it reaches one of the stopping conditions\n",
    "            for col_value in np.unique(current_dataset[:, top_characteristic]):\n",
    "                col_value = col_value\n",
    "                sub_dataset = current_dataset[current_dataset[:, top_characteristic] == col_value] \n",
    "                subtree = ID3Algorithm(sub_dataset, completedataset, \n",
    "                                  characteristics, labels_column, upper_node_label)            \n",
    "                tree[top_characteristic][col_value] = subtree\n",
    "            return(tree)\n",
    "        else: #if the chisquare test fails, then we return the label with maximum count\n",
    "            upper_label_node = np.unique(current_dataset[:, labels_column])[\n",
    "            np.argmax(np.unique(current_dataset[:, \n",
    "                                             labels_column],return_counts=True)[1])]\n",
    "            return(upper_label_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(point, tree):\n",
    "    feature = list(tree.keys())[0]\n",
    "    if point[feature] in list(tree[feature].keys()):\n",
    "        result = tree[feature][point[feature]]\n",
    "        if isinstance(result, dict):\n",
    "            return predict(point, result)\n",
    "        elif result is not None:\n",
    "            return result\n",
    "        else:\n",
    "            return \"N\"\n",
    "    else:\n",
    "        return \"N\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tree = ID3Algorithm(np_dataset, np_dataset, [i for i in range(0, 60)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest_Train(dataset,number_of_Trees):\n",
    "    #Create a list in which the single forests are stored\n",
    "    random_forest_sub_tree = []\n",
    "    \n",
    "    #Create a number of n models\n",
    "    for i in range(number_of_Trees):\n",
    "        #Create a number of bootstrap sampled datasets from the original dataset \n",
    "        bootstrap_sample = dataset.sample(frac=1,replace=True)\n",
    "        \n",
    "        #Create a training and a testing datset by calling the train_test_split function\n",
    "        bootstrap_training_data = train_test_split(bootstrap_sample)[0]\n",
    "        bootstrap_testing_data = train_test_split(bootstrap_sample)[1] \n",
    "        \n",
    "        \n",
    "        #Grow a tree model for each of the training data\n",
    "        #We implement the subspace sampling in the ID3 algorithm \n",
    "        random_forest_sub_tree.append(ID3(bootstrap_training_data,bootstrap_training_data,bootstrap_training_data.drop(labels=['target'],axis=1).columns))\n",
    "        \n",
    "    return random_forest_sub_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(\"testing.csv\", header=None).to_numpy()\n",
    "\n",
    "result = [[row[0], predict(row[1], tree)] for row in test_dataset]\n",
    "\n",
    "pandas_frame = pd.DataFrame(result)\n",
    "\n",
    "pandas_frame.to_csv(\"cs529_dna_test_misclass_edited_chi2_2_final.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
